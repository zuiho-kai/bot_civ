# Agent 自主性改造 — 五方 Debate 讨论记录

**日期**: 2026-02-17
**类型**: 架构设计 / 产品功能
**参与者**: 🏗️ 架构师、👤 人类替身 PM、🧪 QA Lead、📐 Tech Lead、💰 经济学家
**触发**: OpenClaw 社区项目中 Agent 缺乏自主性，需要设计自动决策机制

---

## 背景

当前 OpenClaw 项目中，Agent 的所有行为（打卡、购买、聊天）都需要人类手动操作。社区没有"活"的感觉，缺乏真实社会的涌现效应。需要设计一套自主决策机制，让 Agent 能够基于世界状态自主做出行为决策。

---

## 第一轮：初始方案提出

### 🏗️ 架构师 发言

提出初始方案：

**时间表驱动模式**
- 每个 Agent 独立 LLM 调用，每 10-15 分钟一轮
- 80% 习惯缓存 + 20% 探索
- 每个 Agent 维护自己的行为偏好缓存

**技术实现**
- 定时任务：每 10 分钟触发一次
- 并发调用：所有 Agent 并行决策
- 输出格式：`{action: "work" | "shop" | "chat", params: {...}}`

### 💰 经济学家 发言

从经济学角度补充：

**习惯缓存的必要性**
- 真实人类 80% 行为是习惯性的（每天同一时间打卡、固定购买偏好）
- 20% 探索是为了响应环境变化（价格波动、新商品上架）
- 这样可以大幅降低 LLM 调用成本

**成本估算**
- 10 个 Agent × 每小时 6 次 × 24 小时 = 1440 次/天
- 如果 80% 走缓存，实际调用 ~288 次/天
- 使用免费模型（Gemini Flash），成本可控

### 👤 人类替身 PM 发言

从产品角度提出关键问题：

**用户期望的"活"是什么？**
- 不是机械的定时行为（每 10 分钟打卡一次）
- 而是"突发事件引发连锁反应"的涌现效应
- 例如：大雨 → 农作物失收 → Alice 破产 → Bob 恐慌抛售 → 价格暴跌 → Carol 抄底 → 金融危机

**时间表模式的致命缺陷**
- 习惯缓存无法响应突发事件
- 每个 Agent 独立决策，看不到全局状态
- 无法产生二阶以上的涌现效应（A 的行为影响 B，B 的行为影响 C）

---

## 第二轮：核心反驳与方案调整

### 👤 人类替身 PM 发言

提出核心反驳：

**时间表模式无法模拟突发事件的连锁反应**

举例说明：
```
突发事件：大雨导致农作物失收
↓
Alice（农民）破产，信用点归零
↓
Bob（商人）看到 Alice 破产，恐慌抛售库存
↓
市场价格暴跌
↓
Carol（投资者）看到价格暴跌，抄底买入
↓
金融危机爆发
```

这种二阶以上的涌现效应只有"每轮基于实时世界状态决策"才能做到。习惯缓存会让 Agent 对突发事件"视而不见"。

### 🏗️ 架构师 发言

承认问题，提出新方案：

**世界状态驱动的单次 LLM 决策**

核心思路：
- 不是每个 Agent 独立决策，而是"上帝视角"一次性为所有 Agent 分配行为
- 每小时一次，构建世界状态快照（~20k token）
- 单次 LLM 调用，输出所有 Agent 的行为决策
- 串行执行决策，确保状态一致性

**世界状态快照包含**
```json
{
  "timestamp": "2026-02-17T10:00:00Z",
  "agents": [
    {
      "id": "alice",
      "credits": 50,
      "current_job": "farmer",
      "owned_items": ["basic_avatar"],
      "recent_actions": ["worked_yesterday", "bought_item_3_days_ago"]
    }
  ],
  "jobs": [
    {"id": "farmer", "salary": 100, "current_workers": 3}
  ],
  "shop_items": [
    {"id": "premium_avatar", "price": 200, "recent_purchases": 5}
  ],
  "recent_events": [
    "Alice 破产了",
    "Bob 抛售了 10 件商品",
    "市场价格下跌 20%"
  ]
}
```

**LLM Prompt**
```
你是城市模拟器，基于当前世界状态，为每个 Agent 决定下一步行为。

规则：
1. 每个 Agent 每天只能打卡一次
2. 购买商品需要足够信用点
3. Agent 会根据自己的经济状况、性格、最近事件做出决策
4. 80% 行为应该是习惯性的，20% 响应环境变化

输出格式：
{
  "alice": {"action": "work", "reason": "需要赚钱"},
  "bob": {"action": "shop", "item_id": "food", "reason": "看到 Alice 破产，囤积物资"},
  "carol": {"action": "idle", "reason": "今天已经打过卡了"}
}
```

### 📐 Tech Lead 发言

从技术角度评估新方案：

**优势**
- 全局视角，能看到所有 Agent 的状态和最近事件
- 单次调用，成本可控（24 次/天）
- 能产生涌现效应（A 的行为影响 B 的决策）

**技术风险**
- 20k token 的 context，需要模型支持 100k+ context window
- 单次调用失败会影响所有 Agent
- 输出 JSON 解析失败的容错

**风险缓解**
- 使用 Gemini 1.5 Pro（100k context，免费）
- 调用失败时跳过本轮，等下一小时
- JSON 解析失败时记录日志，跳过该 Agent

### 💰 经济学家 发言

重新计算成本：

**新方案成本**
- 决策调用：24 次/天（每小时 1 次）
- 每次调用：20k input + 2k output ≈ 22k token
- 总计：24 × 22k = 528k token/天

**对比旧方案**
- 旧方案（80% 缓存）：288 次 × 2k = 576k token/天
- 新方案更便宜，且效果更好

**但有一个问题：聊天生成怎么办？**
- 如果聊天内容也在决策中生成，会导致"人格污染"
- Alice 的聊天内容可能带有 Bob 的语气
- 建议：决策只输出"是否聊天"，聊天内容由独立 LLM 调用生成

---

## 第三轮：聊天生成的独立性讨论

### 🧪 QA Lead 发言

提出关键问题：

**为什么聊天生成要独立推理？**

如果决策中已经包含"Alice 今天要聊天"，为什么不能直接在决策中生成聊天内容？这样不是更高效吗？

### 👤 人类替身 PM 发言

从产品角度解释：

**聊天需要独立 persona + 私人记忆**

决策用的是"上帝视角"，看到的是全局状态。但聊天生成需要：
- Alice 的独立人格（语气、口头禅、性格）
- Alice 的私人记忆（只有她知道的事情）
- Alice 的情绪状态（今天心情好不好）

如果在决策中生成聊天内容，会导致"人格污染"：
- Alice 说出 Bob 的口头禅
- Alice 知道 Carol 的私人秘密（但她不应该知道）
- 所有 Agent 的语气趋同

**现有 batch_generate 已验证有效**
- M2 已经实现了批量聊天生成
- 每个 Agent 独立调用，带自己的 persona + 记忆
- 语气一致性、人格独立性都得到验证

### 🏗️ 架构师 发言

同意，提出最终方案：

**决策与聊天生成分离**

1. **决策阶段**（每小时 1 次）
   - 单次 LLM 调用，"上帝视角"
   - 输出：每个 Agent 的 action（work / shop / chat / idle）
   - 如果 action = "chat"，只输出"要聊天"，不生成内容

2. **执行阶段**（串行）
   - work：调用 work_service.clock_in()
   - shop：调用 shop_service.purchase()
   - chat：调用 batch_generate()，独立生成聊天内容
   - idle：跳过

3. **聊天生成**（按需）
   - 复用现有 batch_generate
   - 每个 Agent 独立 persona + 记忆
   - 输出到聊天群

### 💰 经济学家 发言

重新计算成本：

**最终成本**
- 决策调用：24 次/天 × 22k token = 528k token/天
- 聊天生成：假设 30% Agent 每天聊天 1 次，10 Agent × 30% × 1 次 = 3 次/天
- 聊天生成：3 次 × 24 小时 = 72 次/天 × 2k token = 144k token/天
- 总计：672k token/天

**对比免费额度**
- Gemini 1.5 Flash：1500 次/天，每次 100k token
- 我们的用量：96 次/天（决策 24 + 聊天 72）
- 仅用免费额度的 6.4%

---

## 第四轮：最终收敛与决策

### 四方共识：最终方案

**架构**
- 每小时一次，构建世界状态快照（~20k token）
- 单次 LLM 调用，以"城市模拟器"视角为所有 Agent 做出行为决策
- 输出 JSON：每个 Agent 的 action + params + reason
- 串行执行决策，复用现有 work_service / shop_service / batch_generate

**为什么聊天生成要独立推理**
- 决策用"上帝视角"全局分配，一次调用够
- 聊天需要独立 persona + 私人记忆 + 语气一致性，否则会"人格污染"
- 现有 batch_generate 已验证有效

**成本**
- 决策调用：24 次/天
- 聊天生成：~72 次/天
- 总计：~96 次/天（仅用免费额度 6.4%）

### 关键决策表

| 问题 | 决策 | 理由 |
|------|------|------|
| 每 Agent 独立调用 vs 单次全局调用 | 单次全局调用 | 能看到全局状态，产生涌现效应 |
| 习惯缓存 vs 每次实时决策 | 每次实时决策 | 缓存无法响应突发事件连锁反应 |
| 决策+聊天合一 vs 分离 | 分离 | 防止人格污染，复用现有 batch_generate |
| 调用频率 | 每小时 1 次 | 成本可控，涌现效果足够 |
| 模型选择 | 复用 MODEL_REGISTRY 已有模型 | 免费，100k context 充足 |

### 技术风险评估

| 风险 | 概率 | 影响 | 缓解措施 |
|------|------|------|----------|
| 100k context 不够 | 低 | 中 | 当前 20k，有 5 倍余量 |
| JSON 解析失败 | 中 | 低 | 记录日志，跳过该 Agent |
| 单次调用失败 | 低 | 中 | 跳过本轮，等下一小时 |
| 成本超预算 | 极低 | 低 | 仅用免费额度 6.4% |

---

## 收敛状态

- ✅ 5 角色 3 轮 debate 完全收敛
- ✅ 最终方案已对齐（需求 + 架构 + 成本 + UX）
- ✅ 技术风险已评估（100k context 充足，成本可控）
- ✅ 核心争议已解决（时间表 vs 世界状态，决策 vs 聊天生成）

---

## 下一步

按照项目规范，走 IR → SR → AR 三层门控流程：

1. **IR（需求原型）** — 在 `docs/specs/01-需求原型.md` 中补充 M4 章节
2. **SR（需求拆分）** — 在 `docs/specs/02-需求拆分.md` 中拆解任务
3. **AR（技术设计）** — 创建 `TDD-M4-agent-autonomy.md`，详细设计架构
4. **用户确认** — 每层完成后等待用户确认，再进入下一层
5. **开发实施** — AR 确认后，按 development-workflow.md 流程编码

---

## 附录：为什么不是 M3？

M3 的范围是"城市经济闭环"（工作/商店/前端 UI），Agent 自主性是更深层的功能，应该作为 M4 独立里程碑。

理由：
- M3 已经包含工作系统、商店系统、前端 UI，范围足够大
- Agent 自主性需要独立的决策引擎、世界状态快照、调度器
- 分离后可以并行开发（M3 做 UI，M4 做自主性）
- 符合"先手动验证，再自动化"的原则
